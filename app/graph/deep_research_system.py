"""
Sistema de Deep Research - Arquitectura Map-Reduce para InvestigaciÃ³n Paralela
Implementa un equipo especializado: Planner + Workers paralelos + Synthesizer
"""

import asyncio
import logging
from typing import TypedDict, List, Annotated, Dict, Any
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.tools import tool
from langchain_openai import ChatOpenAI
from langchain_community.tools import TavilySearchResults
from langgraph.graph import StateGraph, START, END
from langgraph.constants import Send  # Para Send API
from tavily import TavilyClient
from app.config.settings import LLM_MODEL, TAVILY_API_KEY

logger = logging.getLogger(__name__)

# === ESTADO DEL SISTEMA MAP-REDUCE ===

class DeepResearchState(TypedDict):
    """Estado compartido del sistema Deep Research."""
    research_topic: str
    business_context: Dict[str, Any]  # InformaciÃ³n de la empresa
    research_plan: List[str]  # Lista de consultas de bÃºsqueda generadas por el planner
    research_results: Annotated[List[Dict], lambda x, y: x + y]  # Resultados acumulados de workers
    final_report: str
    execution_summary: str

# === HERRAMIENTAS ESPECIALIZADAS ===

@tool
async def search_web_advanced(query: str) -> Dict[str, Any]:
    """
    Herramienta avanzada de bÃºsqueda web optimizada para investigaciÃ³n empresarial.
    Incluye retry logic y filtrado de resultados.
    """
    try:
        logger.info(f"[Deep Research Worker] ğŸ” Buscando: '{query}'")
        
        # Configurar Tavily con parÃ¡metrso optimizados
        tavily_search = TavilySearchResults(
            max_results=4,  # MÃ¡s resultados para investigaciÃ³n profunda
            search_depth="advanced",  # BÃºsqueda mÃ¡s profunda
            include_answer=True,  # Incluir respuesta directa cuando sea posible
            include_raw_content=False,  # No incluir contenido HTML crudo
            include_images=False  # No necesitamos imÃ¡genes
        )
        
        # Simular tiempo de bÃºsqueda real
        await asyncio.sleep(1)
        
        results = tavily_search.invoke(query)
        
        # Formatear y filtrar resultados
        formatted_results = []
        for result in results:
            if isinstance(result, dict):
                # Filtrar contenido irrelevante o muy corto
                content = result.get("content", "")
                if len(content) > 50:  # Solo resultados con contenido sustancial
                    formatted_results.append({
                        "title": result.get("title", "Sin tÃ­tulo"),
                        "content": content[:800],  # Truncar para evitar overflow
                        "url": result.get("url", ""),
                        "score": result.get("score", 0.0)
                    })
        
        return {
            "query": query,
            "results": formatted_results,
            "results_count": len(formatted_results),
            "status": "success"
        }
        
    except Exception as e:
        logger.error(f"[Deep Research Worker] âŒ Error en bÃºsqueda '{query}': {str(e)}")
        return {
            "query": query,
            "results": [],
            "results_count": 0,
            "status": "error",
            "error": str(e)
        }

# === AGENTES ESPECIALIZADOS ===

class DeepResearchPlanner:
    """El Estratega - Crea planes de investigaciÃ³n detallados."""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=LLM_MODEL, 
            temperature=0.2,  # Baja creatividad para planes consistentes
            max_tokens=300
        )
    
    def create_research_plan(self, research_topic: str, business_context: Dict[str, Any]) -> List[str]:
        """Crea un plan de investigaciÃ³n estructurado."""
        try:
            empresa = business_context.get("nombre_empresa", "la empresa")
            sector = business_context.get("sector", business_context.get("productos_servicios_principales", ""))
            ubicacion = business_context.get("ubicacion", "")
            productos = business_context.get("productos_servicios_principales", "")
            desafios = business_context.get("desafios_principales", "")
            
            planning_prompt = f"""Eres un Estratega de InvestigaciÃ³n Empresarial experto en PYMEs.

CONTEXTO EMPRESARIAL:
- Empresa: {empresa}
- Sector: {sector}
- UbicaciÃ³n: {ubicacion}
- Productos/Servicios: {productos}
- DesafÃ­os principales: {desafios}

SOLICITUD DE INVESTIGACIÃ“N: {research_topic}

TU TAREA: Crear un plan de investigaciÃ³n de 4-5 consultas de bÃºsqueda especÃ­ficas y estratÃ©gicas.

CRITERIOS PARA LAS CONSULTAS:
1. EspecÃ­ficas al sector y ubicaciÃ³n de la empresa
2. Orientadas a resultados accionables
3. Balanceadas entre oportunidades y desafÃ­os
4. Incluir anÃ¡lisis competitivo cuando sea relevante
5. Considerar tendencias actuales del mercado

FORMATO DE RESPUESTA:
Devuelve SOLAMENTE una lista de consultas, una por lÃ­nea, sin numeraciÃ³n ni viÃ±etas.

EJEMPLO:
tendencias mercado restaurantes Lima 2024 post pandemia
competidores directos pollerÃ­as zona Lima Norte anÃ¡lisis
oportunidades delivery comida peruana mercado emergente
estrategias marketing digital restaurantes familiares Ã©xito
proveedores pollo Lima precios mayoristas comparativa

IMPORTANTE: Consultas especÃ­ficas, accionables y relevantes para {empresa}."""

            response = self.llm.invoke([{"role": "user", "content": planning_prompt}])
            
            # Procesar la respuesta para extraer consultas
            queries = []
            for line in response.content.split('\n'):
                line = line.strip()
                if line and len(line) > 10:  # Filtrar lÃ­neas vacÃ­as o muy cortas
                    # Limpiar posibles numeraciones o viÃ±etas
                    clean_line = line.lstrip('123456789.- â€¢â–ªâ–«')
                    if clean_line:
                        queries.append(clean_line.strip())
            
            # Asegurar que tengamos entre 4-6 consultas
            if len(queries) < 4:
                # Agregar consultas genÃ©ricas si no hay suficientes
                queries.extend([
                    f"oportunidades crecimiento {sector} {ubicacion} 2024",
                    f"tendencias consumidor {productos} mercado actual",
                    f"estrategias exitosas PYMES {sector} casos estudio"
                ])
            
            # Limitar a mÃ¡ximo 6 consultas para evitar sobrecarga
            final_queries = queries[:6]
            
            logger.info(f"[Deep Research Planner] ğŸ“‹ Plan creado: {len(final_queries)} consultas")
            for i, query in enumerate(final_queries, 1):
                logger.info(f"  {i}. {query}")
            
            return final_queries
            
        except Exception as e:
            logger.error(f"[Deep Research Planner] âŒ Error creando plan: {str(e)}")
            # Plan de respaldo
            return [
                f"oportunidades mercado {business_context.get('sector', 'negocio')} {business_context.get('ubicacion', 'Peru')}",
                f"tendencias {business_context.get('sector', 'industria')} 2024",
                f"competidores {business_context.get('productos_servicios_principales', 'productos')} anÃ¡lisis",
                f"estrategias crecimiento PYMES {business_context.get('sector', 'pequeÃ±os negocios')}"
            ]

class DeepResearchSynthesizer:
    """El Sintetizador - Combina y analiza todos los resultados."""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model=LLM_MODEL,
            temperature=0.3,  # Algo de creatividad para sÃ­ntesis
            max_tokens=500    # Informe mÃ¡s extenso
        )
    
    def synthesize_research(self, research_topic: str, business_context: Dict[str, Any], 
                          research_results: List[Dict]) -> str:
        """Sintetiza todos los resultados en un informe ejecutivo."""
        try:
            empresa = business_context.get("nombre_empresa", "la empresa")
            
            # Formatear resultados de investigaciÃ³n
            formatted_findings = []
            total_sources = 0
            
            for result in research_results:
                if result.get("status") == "success" and result.get("results"):
                    query = result["query"]
                    findings = result["results"]
                    total_sources += len(findings)
                    
                    # Resumir hallazgos por consulta
                    query_summary = f"**{query}:**\n"
                    for finding in findings[:2]:  # Top 2 resultados por consulta
                        content = finding.get("content", "")[:200]  # Resumir contenido
                        query_summary += f"- {content}...\n"
                    
                    formatted_findings.append(query_summary)
            
            all_findings_text = "\n".join(formatted_findings)
            
            synthesis_prompt = f"""Eres un Consultor Senior especializado en anÃ¡lisis de investigaciÃ³n empresarial.

EMPRESA: {empresa}
SOLICITUD ORIGINAL: {research_topic}

HALLAZGOS DE INVESTIGACIÃ“N ({total_sources} fuentes consultadas):
{all_findings_text}

TU TAREA: Crear un informe ejecutivo conciso y accionable.

ESTRUCTURA DEL INFORME:
1. **RESUMEN EJECUTIVO** (2-3 lÃ­neas clave)
2. **OPORTUNIDADES IDENTIFICADAS** (3-4 puntos especÃ­ficos)
3. **RECOMENDACIONES PRIORITARIAS** (3-4 acciones concretas)
4. **PRÃ“XIMOS PASOS** (2-3 acciones inmediatas)

CRITERIOS:
- Enfoque en insights accionables para {empresa}
- Recomendaciones especÃ­ficas y prÃ¡cticas
- Priorizar por impacto potencial
- Incluir mÃ©tricas o KPIs cuando sea posible
- Tono profesional pero accesible

LÃMITES:
- MÃ¡ximo 400 palabras
- Usar viÃ±etas para mayor claridad
- Evitar jerga tÃ©cnica excesiva"""

            response = self.llm.invoke([{"role": "user", "content": synthesis_prompt}])
            
            # Agregar metadata del proceso
            execution_info = f"\n\n---\n*InvestigaciÃ³n completada: {len(research_results)} consultas ejecutadas, {total_sources} fuentes analizadas*"
            
            final_report = response.content + execution_info
            
            logger.info(f"[Deep Research Synthesizer] ğŸ“Š Informe final generado: {len(final_report)} caracteres")
            
            return final_report
            
        except Exception as e:
            logger.error(f"[Deep Research Synthesizer] âŒ Error en sÃ­ntesis: {str(e)}")
            return f"La investigaciÃ³n sobre '{research_topic}' encontrÃ³ informaciÃ³n relevante, pero hubo un error procesando el informe final. Se recomienda revisar los hallazgos individualmente."

# === NODOS DEL GRAFO MAP-REDUCE CON SEND API ===

def planner_node(state: DeepResearchState) -> Dict[str, Any]:
    """Nodo del Planner - Crea el plan de investigaciÃ³n."""
    logger.info("[Deep Research System] ğŸ§  PLANNER: Creando plan de investigaciÃ³n...")
    
    planner = DeepResearchPlanner()
    research_plan = planner.create_research_plan(
        state["research_topic"], 
        state["business_context"]
    )
    
    return {"research_plan": research_plan}

async def research_worker_node(state: Dict[str, str]) -> Dict[str, List[Dict]]:
    """Nodo WORKER - Ejecuta investigaciÃ³n individual (PARALELO con Send API)."""
    query = state["query"]
    logger.info(f"[Deep Research System] ğŸ” WORKER: Ejecutando investigaciÃ³n para '{query}'")
    
    # Ejecutar bÃºsqueda usando la herramienta asÃ­ncrona
    try:
        search_result = await search_web_advanced.ainvoke({"query": query})
        return {"research_results": [search_result]}
    except Exception as e:
        logger.error(f"[Deep Research System] âŒ Error en worker para '{query}': {str(e)}")
        # Retornar resultado de error
        error_result = {
            "query": query,
            "status": "error",
            "error": str(e),
            "results": [],
            "results_count": 0
        }
        return {"research_results": [error_result]}

def synthesizer_node(state: DeepResearchState) -> Dict[str, Any]:
    """Nodo REDUCE - Sintetiza todos los resultados."""
    logger.info("[Deep Research System] ğŸ“Š SYNTHESIZER: Creando informe final...")
    
    synthesizer = DeepResearchSynthesizer()
    final_report = synthesizer.synthesize_research(
        state["research_topic"],
        state["business_context"],
        state["research_results"]
    )
    
    # Crear resumen de ejecuciÃ³n
    successful_searches = sum(1 for r in state["research_results"] if r.get("status") == "success")
    total_searches = len(state["research_results"])
    
    execution_summary = f"InvestigaciÃ³n completada: {successful_searches}/{total_searches} bÃºsquedas exitosas"
    
    return {
        "final_report": final_report,
        "execution_summary": execution_summary
    }

# === CONSTRUCCIÃ“N DEL GRAFO MAP-REDUCE CON SEND API ===

def create_deep_research_system():
    """
    Crea el sistema completo de Deep Research con arquitectura Map-Reduce usando Send API.
    
    Flujo: START -> planner -> [workers paralelos] -> synthesizer -> END
    """
    logger.info("ğŸ—ï¸ [Deep Research System] Construyendo grafo Map-Reduce con Send API...")
    
    workflow = StateGraph(DeepResearchState)
    
    # === NODOS ===
    workflow.add_node("planner", planner_node)
    workflow.add_node("research_worker", research_worker_node)
    workflow.add_node("synthesizer", synthesizer_node)
    
    # === FLUJO CON SEND API ===
    workflow.set_entry_point("planner")
    
    # SEND API: Desde planner distribuir a workers paralelos
    def continue_to_workers(state: DeepResearchState) -> List[Send]:
        """FunciÃ³n que distribuye las consultas a workers paralelos usando Send API."""
        research_plan = state["research_plan"]
        logger.info(f"[Deep Research System] ğŸ—‚ï¸ DISTRIBUYENDO: {len(research_plan)} tareas a workers usando Send API...")
        
        return [
            Send("research_worker", {"query": query})
            for query in research_plan
        ]
    
    workflow.add_conditional_edges(
        "planner",
        continue_to_workers,
        ["research_worker"]
    )
    
    # WORKERS van a SYNTHESIZER - reduce automÃ¡tico
    workflow.add_edge("research_worker", "synthesizer")  
    workflow.add_edge("synthesizer", END)
    
    logger.info("âœ… [Deep Research System] Grafo Map-Reduce con Send API construido exitosamente")
    
    return workflow.compile()

# === FUNCIÃ“N DE INTERFAZ PRINCIPAL ===

async def perform_deep_research_analysis(research_topic: str, business_context: Dict[str, Any]) -> Dict[str, Any]:
    """
    FunciÃ³n principal para ejecutar investigaciÃ³n profunda.
    
    Args:
        research_topic: Tema de investigaciÃ³n solicitado
        business_context: InformaciÃ³n de la empresa
        
    Returns:
        Dict con el informe final y metadatos de ejecuciÃ³n
    """
    try:
        logger.info(f"ğŸš€ [Deep Research System] Iniciando investigaciÃ³n: '{research_topic}'")
        
        # Crear el grafo del sistema
        research_system = create_deep_research_system()
        
        # Preparar estado inicial
        initial_state = {
            "research_topic": research_topic,
            "business_context": business_context,
            "research_results": []
        }
        
        # Ejecutar el sistema completo
        final_state = await research_system.ainvoke(initial_state)
        
        # Extraer resultados
        result = {
            "success": True,
            "final_report": final_state.get("final_report", ""),
            "execution_summary": final_state.get("execution_summary", ""),
            "research_plan": final_state.get("research_plan", []),
            "total_sources": sum(
                r.get("results_count", 0) 
                for r in final_state.get("research_results", [])
            )
        }
        
        logger.info(f"âœ… [Deep Research System] InvestigaciÃ³n completada exitosamente")
        logger.info(f"   ğŸ“Š Plan: {len(result['research_plan'])} consultas")
        logger.info(f"   ğŸ“š Fuentes: {result['total_sources']} fuentes analizadas")
        
        return result
        
    except Exception as e:
        logger.error(f"âŒ [Deep Research System] Error en investigaciÃ³n: {str(e)}")
        return {
            "success": False,
            "final_report": f"Hubo un error durante la investigaciÃ³n profunda de '{research_topic}'. Error: {str(e)}",
            "execution_summary": "Error en la ejecuciÃ³n",
            "research_plan": [],
            "total_sources": 0
        } 